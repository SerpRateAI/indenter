{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Indenter","text":"<p>Streamlining nano\u2011indentation data processing workflows.</p> <p> </p> <p>Indenter is a library to streamline the workflow of nano\u2011indentation experiment data processing. It provides five core modules:</p> <ul> <li><code>load_datasets</code>: Read and parse measurement files into pandas DataFrames.</li> <li><code>preprocess</code>: Clean, filter, and normalize raw indentation data.</li> <li><code>locate</code>: Identify and extract pop\u2011in events.</li> <li><code>statistics</code>: Perform statistical analysis and model fitting.</li> <li><code>make_dataset</code>: Combine metadata and features into cohesive datasets.</li> </ul> <p>For a quick overview, see the Quickstart.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please file issues and submit pull requests on GitHub.</p> <p>Before submitting a PR:</p> <ol> <li>Fork the repository.</li> <li>Create a feature branch (<code>git checkout -b feature/foo</code>).</li> <li>Commit your changes (<code>git commit -m \"feat: add bar\"</code>).</li> <li>Push to the branch (<code>git push origin feature/foo</code>).</li> <li>Open a pull request.</li> </ol> <p>This project is licensed under the GNU GPL v3.0\u2014see LICENSE.</p>"},{"location":"installation/","title":"Installation","text":"<p>Install from PyPI (once published):</p> <pre><code>pip install indenter\n</code></pre> <p>Or for development from source:</p> <pre><code>git clone https://github.com/SerpRateAI/indenter.git\ncd indenter\npip install -e .\n</code></pre> <p>Indenter supports Python 3.10+ and depends on: - <code>numpy</code> - <code>pandas</code> These are installed automatically via <code>pip</code>.</p>"},{"location":"installation/#testing","title":"Testing","text":"<ol> <li>Install development requirements:</li> </ol> <pre><code>   pip install -e '.[dev]'\n</code></pre> <ol> <li>Run tests:</li> </ol> <pre><code>   pytest\n</code></pre> <ol> <li>Run tests with coverage:</li> </ol> <pre><code>    pytest --cov=indenter --cov-report=term-missing\n</code></pre> <ol> <li>Run tests with coverage and generate HTML report:</li> </ol> <pre><code>    pytest --cov=indenter --cov-report=html\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<pre><code>from pathlib import Path\nfrom indenter.load_datasets import load_txt, load_tdm\n\n# 1) Load indentation data:\ndata_file = Path(\"data/experiment1.txt\")\ndf = load_txt(data_file)\n\n# The indentation data is stored as multiple rows with their respective columns\nprint(df.head())\nprint(\"Timestamp:\", df.attrs['timestamp'])\nprint(\"Number of Points:\", df.attrs['num_points'])\n\n# 2) Load tdm metadata:\ntdm_meta_file = Path(\"data/experiment1.tdm\")\n\n# Load tdm metadata and channels this will create dataframe for root and channels\ndf_tdm_meta_root, df_tdm_meta_channels = load_tdm(tdm_meta_file)\n\n# The root metadata is stored as one row with their respective columns\nprint(df_tdm_meta_root.head())\n\n# To be able to read all the columns of root metadata dataframe it can be transposed\ndf_tdm_meta_root = df_tdm_meta_root.T.reset_index()\ndf_tdm_meta_root.columns = ['attribute', 'value']\nprint(df_tdm_meta_root.head(50))\n\n# The chanel metadata is stored as multiple rows with their respective columns\nprint(df_tdm_meta_channels.head(50))\n</code></pre>"},{"location":"reference/indenter.load_datasets/","title":"load_datasets","text":""},{"location":"reference/indenter.load_datasets/#indenter.load_datasets--load_datasetspy","title":"load_datasets.py","text":"<p>Read indentation experiment TXT data and metadata files into pandas DataFrames. Provides:   - load_txt: load a .txt data file (auto-detect columns) into a DataFrame, with header attrs   - load_tdm: load a .tdm/.tdx metadata file (full channel list) into two DataFrames (root info and channel list)</p> Usage <p>from indenter.load_datasets import load_txt, load_tdm</p>"},{"location":"reference/indenter.load_datasets/#indenter.load_datasets.load_tdm","title":"<code>load_tdm(filepath)</code>","text":"<p>Load a .tdm metadata file into two DataFrames. Args:     filepath: Path to the .tdm/.tdx file. Returns:   - df_root: one row containing       * name, description, title, author       * every instance\u2010attribute under    - df_channels: one row per  with:       group, channel_id, name, unit, description, datatype,       sequence_id, block_id, block_length, value_type Raises:     FileNotFoundError: If the file does not exist. Source code in <code>src/indenter/load_datasets.py</code> <pre><code>def load_tdm(filepath: Path):\n    \"\"\"\n    Load a .tdm metadata file into two DataFrames.\n    Args:\n        filepath: Path to the .tdm/.tdx file.\n    Returns:\n      - df_root: one row containing\n          * name, description, title, author\n          * every instance\u2010attribute under &lt;instance_attributes&gt;\n      - df_channels: one row per &lt;tdm_channel&gt; with:\n          group, channel_id, name, unit, description, datatype,\n          sequence_id, block_id, block_length, value_type\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    if not filepath.is_file():\n        raise FileNotFoundError(f\"TDM file not found: {filepath}\")\n    tree = ET.parse(str(filepath))\n    root = tree.getroot()\n    ns = {\"usi\": \"http://www.ni.com/Schemas/USI/1_0\"}\n\n    # --- extract tdm_root info ---\n    tr = root.find(\".//tdm_root\")\n    root_info = {\n        \"root_name\":        tr.findtext(\"name\"),\n        \"root_description\": tr.findtext(\"description\"),\n        \"root_title\":       tr.findtext(\"title\"),\n        \"root_author\":      tr.findtext(\"author\"),\n    }\n    inst = tr.find(\"instance_attributes\")\n    for attr in inst:\n        # double_attribute, string_attribute, long_attribute, time_attribute...\n        key = attr.get(\"name\")\n        # string_attribute has &lt;s&gt; contents, others have .text\n        if attr.tag.endswith(\"string_attribute\"):\n            val = attr.findtext(\"s\")\n        else:\n            val = attr.text\n        # strip leading/trailing whitespace (incl. newlines and tabs)\n        val = val.strip() if isinstance(val, str) else val\n        root_info[key] = val\n    df_root = pd.DataFrame([root_info])\n\n    # --- build helper maps for channels ---\n    # 1) group id \u2192 group name\n    group_map = {\n        g.get(\"id\"): g.findtext(\"name\")\n        for g in root.findall(\".//tdm_channelgroup\")\n    }\n    # 2) blocks: inc0, inc1, \u2026 \u2192 length &amp; valueType\n    block_map = {\n        blk.get(\"id\"): {\n            \"block_length\": int(blk.get(\"length\")),\n            \"value_type\":   blk.get(\"valueType\")\n        }\n        for blk in root.findall(\".//usi:include/file/block\", ns)\n    }\n    # 3) sequence \u2192 block: usi1 \u2192 inc0, etc.\n    seq2blk = {\n        seq.get(\"id\"): seq.find(\"values\").get(\"external\")\n        for seq in root.findall(\".//usi:data/*_sequence\", ns)\n    }\n    # 4) channel \u2192 sequence via localcolumn\n    chan2seq = {}\n    for lc in root.findall(\".//localcolumn\"):\n        m1 = re.search(r'id\\(\"([^\"]+)\"\\)', (lc.findtext(\"measurement_quantity\") or \"\"))\n        m2 = re.search(r'id\\(\"([^\"]+)\"\\)', (lc.findtext(\"values\") or \"\"))\n        if m1 and m2:\n            chan2seq[m1.group(1)] = m2.group(1)\n\n    # --- now build per\u2010channel rows ---\n    records = []\n    for c in root.findall(\".//tdm_channel\"):\n        cid = c.get(\"id\")\n        grp_txt = c.findtext(\"group\") or \"\"\n        m = re.search(r'id\\(\"([^\"]+)\"\\)', grp_txt)\n        group = group_map.get(m.group(1)) if m else None\n\n        seq = chan2seq.get(cid)\n        blk = seq2blk.get(seq)\n        rec = {\n            \"group\":        group,\n            \"channel_id\":   cid,\n            \"name\":         c.findtext(\"name\"),\n            \"unit\":         c.findtext(\"unit_string\"),\n            \"description\":  c.findtext(\"description\"),\n            \"datatype\":     c.findtext(\"datatype\"),\n            \"sequence_id\":  seq\n        }\n        records.append(rec)\n\n    df_channels = pd.DataFrame.from_records(records)\n    logger.info(f\"Loaded TDM metadata {filepath.name}: {len(df_channels)} channels\")\n\n    return df_root, df_channels\n</code></pre>"},{"location":"reference/indenter.load_datasets/#indenter.load_datasets.load_txt","title":"<code>load_txt(filepath)</code>","text":"<p>Load a .txt indentation data file into a DataFrame. Automatically detects the header line (column names) and numeric block.</p> <p>Attempts UTF-8 decoding first, falls back to Latin-1 on failure.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path</code> <p>Path to the .txt file.</p> required <p>Returns:     DataFrame with columns from the file, and attrs:       - timestamp: first non-empty line       - num_points: parsed from 'Number of Points = N'       - Depth (nm): parsed from the first column with the name \"Depth (nm)\"        - Load (\u00b5N): parsed from the second column with the name \"Load (uN)\"       - Time (s): parsed from the third column with the name \"Time (s)\" Raises:     FileNotFoundError: If the file does not exist.     NotImplementedError: If the file type is not supported.     UnicodeDecodeError: If the file cannot be decoded with UTF-8 or Latin-1.     ValueError: If no numeric data is found in the file.</p> Source code in <code>src/indenter/load_datasets.py</code> <pre><code>def load_txt(filepath: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a .txt indentation data file into a DataFrame.\n    Automatically detects the header line (column names) and numeric block.\n\n    Attempts UTF-8 decoding first, falls back to Latin-1 on failure.\n\n    Args:\n        filepath: Path to the .txt file.\n    Returns:\n        DataFrame with columns from the file, and attrs:\n          - timestamp: first non-empty line\n          - num_points: parsed from 'Number of Points = N'\n          - Depth (nm): parsed from the first column with the name \"Depth (nm)\" \n          - Load (\u00b5N): parsed from the second column with the name \"Load (uN)\"\n          - Time (s): parsed from the third column with the name \"Time (s)\"\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        NotImplementedError: If the file type is not supported.\n        UnicodeDecodeError: If the file cannot be decoded with UTF-8 or Latin-1.\n        ValueError: If no numeric data is found in the file.\n    \"\"\"\n    # Check if the file exists\n    if not filepath.is_file():\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n\n    # Check file extension if it is a .txt file if not raise NotImplementedError\n    if filepath.suffix.lower() != \".txt\":\n        raise NotImplementedError(f\"File type '{filepath.suffix}' is not supported yet. Only '.txt' files are currently implemented.\")\n\n    # Read lines with encoding fallback\n    try:\n        raw = filepath.read_text(encoding='utf-8')\n    except UnicodeDecodeError:\n        logger.warning(f\"UTF-8 decode failed for {filepath}, falling back to Latin-1\")\n        try:\n            raw = filepath.read_text(encoding='latin1')\n        except Exception as e:\n            logger.error(f\"Latin-1 decode also failed for {filepath}.\")\n            raise e\n    text = raw.splitlines()\n\n    # Extract timestamp and num_points\n    timestamp = None\n    num_points = None\n    for line in text:\n        if timestamp is None and line.strip():\n            timestamp = line.strip()\n        if 'Number of Points' in line and '=' in line:\n            try:\n                num_points = int(line.split('=', 1)[1])\n            except ValueError:\n                pass\n        if timestamp and num_points is not None:\n            break\n\n    # Find start of numeric block: first row where every tab-split token is a number\n    start_idx = None\n    num_re = re.compile(r'^[-+]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?$')\n    for i, line in enumerate(text):\n        tokens = line.strip().split('\\t')\n        if tokens and all(num_re.match(tok) for tok in tokens):\n            start_idx = i\n            break\n    if start_idx is None:\n        raise ValueError(f\"No numeric data found in {filepath}\")\n\n    # Header is the last non-empty line before the numeric block\n    header_idx = start_idx - 1\n    while header_idx &gt;= 0 and not text[header_idx].strip():\n        header_idx -= 1\n    if header_idx &gt;= 0:\n        col_names = text[header_idx].split('\\t')\n    else:\n        col_names = []\n\n    # Load the numeric block with tab delimiter\n    data_str = \"\\n\".join(text[start_idx:])\n    arr = np.loadtxt(StringIO(data_str), delimiter='\\t')\n\n    # Force 2D array\n    if arr.ndim == 0:\n        arr = arr.reshape(1, 1)\n    elif arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n\n    # If header didn\u2019t match, generate generic names\n    if not col_names or len(col_names) != arr.shape[1]:\n        col_names = [f\"col_{i}\" for i in range(arr.shape[1])]\n\n    df = pd.DataFrame(arr, columns=col_names)\n    df.attrs['timestamp'] = timestamp\n    df.attrs['num_points'] = num_points\n    logger.info(f\"Loaded TXT data {filepath.name}: {df.shape[0]}\u2009\u00d7\u2009{df.shape[1]}\")\n    return df\n</code></pre>"},{"location":"reference/indenter.locate/","title":"locate","text":""},{"location":"reference/indenter.make_dataset/","title":"make_dataset","text":""},{"location":"reference/indenter.preprocess/","title":"preprocess","text":""},{"location":"reference/indenter.statistics/","title":"statistics","text":""}]}